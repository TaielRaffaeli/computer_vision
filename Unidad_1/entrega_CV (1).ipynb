{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "fbVJfhVv-g_9",
        "Ls9Wdx2x_PgN",
        "WrIX0k1z_skm",
        "3HvUadxA_hc1",
        "ERBoTXZ6JSB-",
        "M7QGKSwKJcfm",
        "EhyhkeiYFks9",
        "D6VmjpYOFqeU",
        "Pc_hTi9_Ly7F",
        "YbJ_4zl_f-0U",
        "WPe_xqH7gNit"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicios Unidad 1\n",
        "\n",
        "5. (Entrega obligatoria individual en repo) Genere un video en un patio o en un hall de edificio donde en un principio se vea vacío y luego aparezca una persona. Mediante los métodos de motion detection (sin usar deep learning) logre una detección de la persona cuando entra al cuadro suponiendo la utilidad para una cámara de seguridad.\n",
        "Luego sobre el mismo video aplique los algoritmos de flujo denso y disperso que se mostraron en clase.\n",
        "Escriba una reflexión sobre los resultados en el formato md dentro del Jupyter Notebook.\n",
        "6. (Entrega obligatoria individual en repo) Explique cuál es diferencia entre localización de objetos y clasificación de imágenes. Muestre ejemplos de ello.\n"
      ],
      "metadata": {
        "id": "PWN7mmKQdIYE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instalar e importar librerias"
      ],
      "metadata": {
        "id": "fbVJfhVv-g_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapy ultralytics ffmpeg"
      ],
      "metadata": {
        "id": "7nxkk_bydVuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iyp6iVzdbR0i"
      },
      "outputs": [],
      "source": [
        "# Importar librerias para procesar videos e imagenes\n",
        "import cv2\n",
        "\n",
        "# Importar librerias para graficar\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Importar librerias para manipulacion de imagenes\n",
        "import numpy as np\n",
        "\n",
        "# Importar librerias para visualizar y modificar videos\n",
        "import mediapy as media\n",
        "import ffmpeg\n",
        "\n",
        "# Importar librerias para utilizar modelo de deteccion de objetos\n",
        "from ultralytics import YOLO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definir funciones de procesamientos de video"
      ],
      "metadata": {
        "id": "v-LV6Wn9_H19"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Funcion para leer el video y aplicar procesamiento"
      ],
      "metadata": {
        "id": "Ls9Wdx2x_PgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para procesar un video:\n",
        "def video_processor(filename_in, filename_out, process_func, max_time=10, **kwargs):\n",
        "    # Abrir el video de entrada para lectura\n",
        "    with media.VideoReader(filename_in) as r:\n",
        "        # Crear un archivo de video de salida\n",
        "        with media.VideoWriter(filename_out, shape=r.shape, fps=r.fps, bps=r.bps) as w:\n",
        "            count = 0  # Inicializar contador de fotogramas\n",
        "            prev_image = None  # Inicializar la imagen previa\n",
        "\n",
        "            # Iterar sobre cada imagen (fotograma) del video\n",
        "            for image in r:\n",
        "                new_image = media.to_uint8(image)  # Convertir la imagen a formato flotante\n",
        "\n",
        "                # Comprobar si es la primera imagen\n",
        "                if prev_image is None:\n",
        "                    prev_image = new_image.copy()\n",
        "\n",
        "                # Procesar la imagen utilizando la función dada\n",
        "                processed_image = process_func(new_image, prev_image, **kwargs)\n",
        "\n",
        "                # Añadir la imagen procesada al video de salida\n",
        "                w.add_image(processed_image)\n",
        "\n",
        "                # Actualizar la imagen previa\n",
        "                prev_image = new_image.copy()\n",
        "\n",
        "                # Incrementar el contador de fotogramas\n",
        "                count += 1\n",
        "\n",
        "                # Detener el proceso si se alcanza el tiempo máximo\n",
        "                if count >= max_time * r.fps:\n",
        "                    break"
      ],
      "metadata": {
        "id": "rWo786RY-fc1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Funcion para dibujar los contornos en el video"
      ],
      "metadata": {
        "id": "WrIX0k1z_skm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_contours(frame, contours, color=(0, 255, 0), thickness=2):\n",
        "    # Comprobar si la imagen es en escala de grises (1 canal)\n",
        "    if len(frame.shape) == 2 or frame.shape[2] == 1:\n",
        "        # Convertir la imagen de escala de grises a color (3 canales)\n",
        "        result_image = cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR)\n",
        "    else:\n",
        "        # Si ya es una imagen de color, simplemente hacer una copia\n",
        "        result_image = frame.copy()\n",
        "\n",
        "    # Dibujar cada contorno en la imagen\n",
        "    for contour in contours:\n",
        "        # Obtener el rectángulo delimitador para cada contorno\n",
        "        x, y, w, h = cv2.boundingRect(contour)\n",
        "        # Dibujar el rectángulo\n",
        "        cv2.rectangle(result_image, (x, y), (x + w, y + h), color, thickness)\n",
        "\n",
        "    return result_image"
      ],
      "metadata": {
        "id": "e1E9VsuyJBtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Funcion para la deteccion de objetos"
      ],
      "metadata": {
        "id": "3HvUadxA_hc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función actualizada para detectar movimientos y dibujar cuadros delimitadores:\n",
        "def process_frame_difference_full(new_image, prev_image, **kwargs):\n",
        "    # Convertir las imágenes a escala de grises\n",
        "    new_gray = cv2.cvtColor(new_image, cv2.COLOR_RGB2GRAY)\n",
        "    prev_gray = cv2.cvtColor(prev_image, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    # Calcular la diferencia absoluta entre los fotogramas actual y anterior\n",
        "    frame_diff = cv2.absdiff(new_gray, prev_gray)\n",
        "\n",
        "    # Normalizar la imagen de diferencia\n",
        "    norm_diff = cv2.normalize(frame_diff, None, 0, 255, cv2.NORM_MINMAX)\n",
        "\n",
        "    # Umbralizar la imagen para resaltar las diferencias\n",
        "    _, thresh = cv2.threshold(norm_diff, 30, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    # Dilatar la imagen umbralizada para mejorar la detección de contornos\n",
        "    kernel = np.ones((5,5),np.uint8)\n",
        "    dilated = cv2.dilate(thresh, kernel, iterations = 1)\n",
        "\n",
        "    # Convertir la imagen dilatada a formato adecuado para findContours\n",
        "    dilated = dilated.astype(np.uint8)\n",
        "\n",
        "    # Encontrar contornos en la imagen dilatada\n",
        "    contours, _ = cv2.findContours(dilated, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    # Dibujar cuadros delimitadores alrededor de los contornos\n",
        "    if kwargs.get('draw_mode', 0) == 0:\n",
        "      result_image = draw_contours(new_image, contours)\n",
        "    elif kwargs.get('draw_mode', 0) == 1:\n",
        "      result_image = draw_contours(thresh, contours)\n",
        "\n",
        "    return result_image"
      ],
      "metadata": {
        "id": "KqrBoeZK_gQ2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Funcion para la deteccion de objetos con filtro de contornos"
      ],
      "metadata": {
        "id": "ERBoTXZ6JSB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función actualizada para detectar movimientos y dibujar cuadros delimitadores\n",
        "def process_frame_difference_filtro(new_image, prev_image, min_area=3000, **kwargs):\n",
        "    # Convertir las imágenes a escala de grises\n",
        "    new_gray = cv2.cvtColor(new_image, cv2.COLOR_RGB2GRAY)\n",
        "    prev_gray = cv2.cvtColor(prev_image, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    # Calcular la diferencia absoluta entre los fotogramas actual y anterior\n",
        "    frame_diff = cv2.absdiff(new_gray, prev_gray)\n",
        "\n",
        "    # Normalizar la imagen de diferencia\n",
        "    norm_diff = cv2.normalize(frame_diff, None, 50, 255, cv2.NORM_MINMAX)\n",
        "\n",
        "    # Umbralizar la imagen para resaltar las diferencias\n",
        "    _, thresh = cv2.threshold(norm_diff, 80, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    # Dilatar la imagen umbralizada para mejorar la detección de contornos\n",
        "    kernel = np.ones((5,5),np.uint8)\n",
        "    dilated = cv2.dilate(thresh, kernel, iterations=1)\n",
        "\n",
        "    # Convertir la imagen dilatada a formato adecuado para findContours\n",
        "    dilated = dilated.astype(np.uint8)\n",
        "\n",
        "    # Encontrar contornos en la imagen dilatada\n",
        "    contours, _ = cv2.findContours(dilated, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    # Filtrar los contornos por área\n",
        "    filtered_contours = [cnt for cnt in contours if cv2.contourArea(cnt) > min_area]\n",
        "\n",
        "\n",
        "    # Dibujar cuadros delimitadores alrededor de los contornos filtrados\n",
        "    if kwargs.get('draw_mode', 0) == 0:\n",
        "        result_image = draw_contours(new_image, filtered_contours)\n",
        "    elif kwargs.get('draw_mode', 0) == 1:\n",
        "        result_image = draw_contours(thresh, filtered_contours)\n",
        "\n",
        "    return result_image"
      ],
      "metadata": {
        "id": "Pi4FqsTeI_rV"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Funcion para la deteccion de objetos con modelo deep learning"
      ],
      "metadata": {
        "id": "M7QGKSwKJcfm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained YOLOv8 model\n",
        "model = YOLO('yolov8n')  # This will automatically download the model weights\n",
        "\n",
        "# Función para detectar objetos en una imagen:\n",
        "def detect_objects(new_image, prev_image, **kwargs):\n",
        "    # Convertir la imagen a float32\n",
        "    results = model(new_image)\n",
        "\n",
        "    # Clases de interés para graficar\n",
        "    classes = kwargs.get('classes', ['person'])\n",
        "\n",
        "    # Iteramos sobre los boung boxes obtenidos\n",
        "    for box in results[0].boxes:\n",
        "        # Extrayendo los datos del tensor\n",
        "        x1, y1, x2, y2, confidence, cls = box.data[0]\n",
        "\n",
        "        # Obteniendo el nombre de la clase\n",
        "        class_name = model.names[int(cls)]\n",
        "\n",
        "        # Parámetros opcionales del bounding box\n",
        "        color = kwargs.get('color', (0, 255, 0))\n",
        "        thickness = kwargs.get('thickness', 2)\n",
        "\n",
        "        if class_name in classes:\n",
        "            # Dibujar el rectángulo\n",
        "            cv2.rectangle(new_image, (int(x1), int(y1)), (int(x2), int(y2)), color, thickness)\n",
        "\n",
        "            # Agregar el texto de la confianza\n",
        "            confidence_text = f\"{class_name}: {confidence:.2f}\"\n",
        "            cv2.putText(new_image, confidence_text, (int(x1), int(y1)-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "\n",
        "    return new_image\n"
      ],
      "metadata": {
        "id": "UUpt3ElI-cTO"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usar video sin posicion fija"
      ],
      "metadata": {
        "id": "bFpBkk8NDDw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "video_base = '/content/20240408_104646.mp4'"
      ],
      "metadata": {
        "id": "mUPBJOi3DKaU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deteccion de objeto simple"
      ],
      "metadata": {
        "id": "Lbno23VrCzFe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Solo contornos blanco y negro"
      ],
      "metadata": {
        "id": "EhyhkeiYFks9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "video_salida = 'personFrame1.mp4'\n",
        "\n",
        "\n",
        "# Llamar a la función para procesar el video\n",
        "video_processor(video_base, video_salida, process_frame_difference_full,\n",
        "                max_time=10, draw_mode=1)\n",
        "\n",
        "# Mostrar el video resultante\n",
        "media.show_video(media.read_video(video_salida), fps=30)"
      ],
      "metadata": {
        "id": "tYuxwWcFC370"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Aplicado a la imagen a color"
      ],
      "metadata": {
        "id": "D6VmjpYOFqeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Llamar a la función para procesar el video\n",
        "video_salida = 'personFrame0.mp4'\n",
        "\n",
        "video_processor(video_base, video_salida, process_frame_difference_full,\n",
        "                max_time=10, draw_mode=0)\n",
        "\n",
        "# Mostrar el video resultante\n",
        "media.show_video(media.read_video(video_salida), fps=30)"
      ],
      "metadata": {
        "id": "0z0sxoJBFkNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Deteccion de objetos con filtro de contornos"
      ],
      "metadata": {
        "id": "Pc_hTi9_Ly7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Llamar a la función para procesar el video\n",
        "video_salida = 'fullPersonFiltro.mp4'\n",
        "\n",
        "video_processor(video_base, video_salida, process_frame_difference_filtro,\n",
        "                max_time=10, draw_mode=0)\n",
        "\n",
        "# Mostrar el video resultante\n",
        "media.show_video(media.read_video(video_salida), fps=30)"
      ],
      "metadata": {
        "id": "dahxLY0BI5CF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "YOLO_person = 'person_YOLO.mp4'\n",
        "\n",
        "parameters = dict(classes=['person'])    # ['bus', 'car', 'truck']\n",
        "\n",
        "# Llamar a la función para procesar el video\n",
        "video_processor(video_base, YOLO_person, detect_objects, **parameters)\n",
        "\n",
        "# Mostrar el video resultante\n",
        "media.show_video(media.read_video(YOLO_person), fps=30)"
      ],
      "metadata": {
        "id": "rrG4xz6NLand"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusiones de la deteccion de objetos con video a mano\n",
        "## Deteccion de objetos simple\n",
        "Al aplicar las funciones de detección de objetos en un video sin una posición fija (cámara en mano), se detectan muchos contornos innecesarios que únicamente \"ensucian la detección de los objetos importantes\". Esto se puede corregir aplicando un filtro exhaustivo de contornos que estén dentro de un rango de valores específico y ajustando el umbralado, además de emplear otras técnicas de filtrado más avanzadas. Aun así, el algoritmo sigue detectando contornos que no corresponden a la persona, lo que lo hace ineficiente en este tipo de video.\n",
        "\n",
        "## Deteccion de objetos con modelo de deep learning (extra)\n",
        "En cuanto al modelo YOLO, al aplicarlo en el video se obtienen resultados excelentes, a pesar de que el video no esté grabado desde una posición fija."
      ],
      "metadata": {
        "id": "6vJvz2XFP3qs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usar video con posicion fija"
      ],
      "metadata": {
        "id": "29CFdEfPf22N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "video_base = '/content/video_fijo.mp4'"
      ],
      "metadata": {
        "id": "heuAJkzNpMKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Solo contornos blanco y negro"
      ],
      "metadata": {
        "id": "EMEYc99TfU2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "video_salida = 'PRUEBAFrame1.mp4'\n",
        "\n",
        "\n",
        "# Llamar a la función para procesar el video\n",
        "video_processor(video_base, video_salida, process_frame_difference_full,\n",
        "                max_time=10, draw_mode=1)\n",
        "\n",
        "# Mostrar el video resultante\n",
        "media.show_video(media.read_video(video_salida), fps=30)"
      ],
      "metadata": {
        "id": "UNHnXrgVTp98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Deteccion de objetos con filtro de contornos en blanco y negro"
      ],
      "metadata": {
        "id": "YbJ_4zl_f-0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Llamar a la función para procesar el video\n",
        "video_salida = 'PRUEBAFrame2.mp4'\n",
        "\n",
        "video_processor(video_base, video_salida, process_frame_difference_filtro,\n",
        "                max_time=10, draw_mode=1)\n",
        "\n",
        "# Mostrar el video resultante\n",
        "media.show_video(media.read_video(video_salida), fps=30)"
      ],
      "metadata": {
        "id": "HiwEdQkcZf6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Deteccion de objetos con filtro de contornos en imagen real"
      ],
      "metadata": {
        "id": "WPe_xqH7gNit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Llamar a la función para procesar el video\n",
        "video_salida = 'PRUEBAFrame3.mp4'\n",
        "\n",
        "video_processor(video_base, video_salida, process_frame_difference_filtro,\n",
        "                max_time=10, draw_mode=0)\n",
        "\n",
        "# Mostrar el video resultante\n",
        "media.show_video(media.read_video(video_salida), fps=30)"
      ],
      "metadata": {
        "id": "ZaPsKXjbgMt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusiones de la deteccion de objetos con video fijo\n",
        "## Deteccion de objetos simple\n",
        "En este otro caso, se aplicaron las mismas funciones de procesamiento de video para la detección de objetos y el rendimiento fue mucho más óptimo sin aplicar filtros de contornos. Al aplicar los filtros, la detección de objetos es mucho más limpia y casi tan óptima como el modelo YOLO aplicado anteriormente."
      ],
      "metadata": {
        "id": "81HRJRkcmg-1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Funciones de flujo denso y disperso"
      ],
      "metadata": {
        "id": "WgwHVgy5oC8c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Funcion de flujo disperso"
      ],
      "metadata": {
        "id": "BZ8htHAPoQXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_sparse_optical_flow(new_image, prev_image):\n",
        "    # Preparamos las imagenes de trabajo\n",
        "    new_gray = cv2.cvtColor(new_image, cv2.COLOR_BGR2GRAY)\n",
        "    prev_gray_image = cv2.cvtColor(prev_image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Verificar si ya se han detectado las características de Shi-Tomasi\n",
        "    if not hasattr(process_sparse_optical_flow, \"shi_tomasi_done\"):\n",
        "        # Definir parámetros para la detección de esquinas de Shi-Tomasi\n",
        "        feature_params = dict(maxCorners=300, qualityLevel=0.2, minDistance=2, blockSize=7)\n",
        "        # Detectar puntos característicos en la imagen\n",
        "        process_sparse_optical_flow.prev_points = cv2.goodFeaturesToTrack(new_gray, mask=None, **feature_params)\n",
        "        # Crear una máscara para dibujar el flujo óptico\n",
        "        process_sparse_optical_flow.mask = np.zeros_like(new_image)\n",
        "        # Marcar que se ha completado la detección de Shi-Tomasi\n",
        "        process_sparse_optical_flow.shi_tomasi_done = True\n",
        "\n",
        "    # Continuar si se ha completado la detección de Shi-Tomasi\n",
        "    if process_sparse_optical_flow.shi_tomasi_done:\n",
        "        prev_points = process_sparse_optical_flow.prev_points\n",
        "        mask = process_sparse_optical_flow.mask\n",
        "\n",
        "    # Parámetros para el flujo óptico de Lucas-Kanade\n",
        "    lk_params = dict(winSize=(15, 15), maxLevel=2,\n",
        "                     criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
        "\n",
        "    # Calcular el flujo óptico de Lucas-Kanade\n",
        "    new_points, status, error = cv2.calcOpticalFlowPyrLK(prev_gray_image, new_gray, prev_points, None, **lk_params)\n",
        "    # Filtrar puntos buenos\n",
        "    good_old = prev_points[status == 1]\n",
        "    good_new = new_points[status == 1]\n",
        "    color = (0, 255, 0)  # Color para el dibujo\n",
        "    # Dibujar el movimiento (flujo óptico)\n",
        "    for i, (new, old) in enumerate(zip(good_new, good_old)):\n",
        "        a, b = new.astype(int).ravel()\n",
        "        c, d = old.astype(int).ravel()\n",
        "        mask = cv2.line(mask, (a, b), (c, d), color, 2)\n",
        "        new_image = cv2.circle(new_image, (a, b), 3, color, -1)\n",
        "\n",
        "    # Combinar la imagen actual con las líneas de flujo óptico dibujadas\n",
        "    output = cv2.add(new_image, mask)\n",
        "    # Actualizar puntos para el siguiente cuadro\n",
        "    process_sparse_optical_flow.prev_points = good_new.reshape(-1, 1, 2)\n",
        "    return output"
      ],
      "metadata": {
        "id": "RmRAkIAZn62-"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Funcion de flujo denso"
      ],
      "metadata": {
        "id": "J6KCxkJToU5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para procesar el flujo óptico denso\n",
        "def process_dense_optical_flow(new_image, prev_image):\n",
        "    # Convierte la nueva imagen a escala de grises\n",
        "    gray = cv2.cvtColor(new_image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    if not hasattr(process_dense_optical_flow, \"init_done\"):\n",
        "        process_dense_optical_flow.prev_gray = cv2.cvtColor(new_image, cv2.COLOR_BGR2GRAY)\n",
        "        process_dense_optical_flow.mask = np.zeros_like(new_image)\n",
        "        process_dense_optical_flow.mask[..., 1] = 255\n",
        "        process_dense_optical_flow.init_done = True\n",
        "\n",
        "    if process_dense_optical_flow.init_done:\n",
        "        prev_gray = process_dense_optical_flow.prev_gray\n",
        "        mask = process_dense_optical_flow.mask\n",
        "\n",
        "    # Calcula el flujo óptico\n",
        "    flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "    # Computa magnitud y ángulo de los vectores 2D\n",
        "    magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
        "    # Establece el tono de la imagen según la dirección del flujo óptico\n",
        "    mask[..., 0] = angle * 180 / np.pi / 2\n",
        "    # Establece el valor de la imagen según la magnitud del flujo óptico\n",
        "    mask[..., 2] = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)\n",
        "    # Convierte de HSV a RGB\n",
        "    rgb = cv2.cvtColor(mask, cv2.COLOR_HSV2BGR)\n",
        "    # Actualiza la imagen previa a gris\n",
        "    process_dense_optical_flow.prev_grayprev_gray = gray.copy()\n",
        "    return rgb"
      ],
      "metadata": {
        "id": "Vfa7Iz0gn_5u"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definir videos base"
      ],
      "metadata": {
        "id": "VKV1KU8-o75n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "video_base_MANO = '/content/20240408_104646.mp4'\n",
        "\n",
        "video_base_FIJO = '/content/video_fijo.mp4'"
      ],
      "metadata": {
        "id": "jioXShE6o_UH"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aplicacion de flujo disperso"
      ],
      "metadata": {
        "id": "cjGJ1WVroXrW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Video en mano"
      ],
      "metadata": {
        "id": "-XO4Yy3tofXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Nombres de los archivos de video de entrada y salida\n",
        "video_salida = 'MANO_sparse_optical_flow.mp4'\n",
        "\n",
        "# Llamar a la función para procesar el video\n",
        "video_processor(video_base_MANO, video_salida, process_sparse_optical_flow,\n",
        "                max_time=10)\n",
        "\n",
        "# Mostrar el video resultante\n",
        "media.show_video(media.read_video(video_salida), fps=30)"
      ],
      "metadata": {
        "id": "YnfjP03poscG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Video fijo"
      ],
      "metadata": {
        "id": "d2pTwnDLoiiN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Nombres de los archivos de video de entrada y salida\n",
        "video_salida = 'FIJO_sparse_optical_flow.mp4'\n",
        "\n",
        "# Llamar a la función para procesar el video\n",
        "video_processor(video_base_FIJO, video_salida, process_sparse_optical_flow,\n",
        "                max_time=10)\n",
        "\n",
        "# Mostrar el video resultante\n",
        "media.show_video(media.read_video(video_salida), fps=30)"
      ],
      "metadata": {
        "id": "BUxJ61hTos--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aplicacion de flujo denso"
      ],
      "metadata": {
        "id": "eMwWYIcgoliO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Video en mano"
      ],
      "metadata": {
        "id": "6uunnmYionIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Nombres de los archivos de video de entrada y salida\n",
        "video_salida = 'MANO_dense_optical_flow.mp4'\n",
        "\n",
        "# Llamar a la función para procesar el video\n",
        "video_processor(video_base_MANO, video_salida, process_dense_optical_flow,\n",
        "                max_time=20)\n",
        "\n",
        "# Mostrar el video resultante\n",
        "media.show_video(media.read_video(video_salida), fps=30)"
      ],
      "metadata": {
        "id": "yWMmmeNoowd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Video fijo"
      ],
      "metadata": {
        "id": "o3CPqu12oooe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Nombres de los archivos de video de entrada y salida\n",
        "video_salida = 'FIJO_dense_optical_flow.mp4'\n",
        "\n",
        "# Llamar a la función para procesar el video\n",
        "video_processor(video_base_FIJO, video_salida, process_dense_optical_flow,\n",
        "                max_time=20)\n",
        "\n",
        "# Mostrar el video resultante\n",
        "media.show_video(media.read_video(video_salida), fps=30)"
      ],
      "metadata": {
        "id": "Ed80pZZcoxF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusiones de la aplicacion de funciones de flujo denso y disperso\n",
        "\n",
        "## Flujo disperso\n",
        "Al aplicar la función de flujo disperso en ambos videos, funciona de manera clara y se logra ver su aplicación. En ambos videos, hay detecciones de movimiento falsas, pero cuando aparece la persona, se puede visualizar cómo los keypoints \"acompañan\" la figura de la persona.\n",
        "\n",
        "## Flujo denso\n",
        "En cuanto a la aplicación de la función de flujo denso, en ambos videos funciona mal, no siendo clara la visualización del movimiento y generando confusión cuando la persona entra en escena.\n",
        "\n",
        "El flujo disperso en este caso es el que mejor funciona (entre flujo denso y disperso), siendo clara su aplicación y observando una diferencia de movimiento cuando la persona entra en escena."
      ],
      "metadata": {
        "id": "Xy9dcnamyzrm"
      }
    }
  ]
}